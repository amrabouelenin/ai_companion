from fastapi import FastAPI, HTTPException, Request, WebSocket, WebSocketDisconnect, Depends, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
import io
import urllib.parse
from pydantic import BaseModel
import httpx
import os
import json
from typing import List, Dict, Optional, Any
import asyncio
import time

from modes.mode_manager import ModeManager
from services.tts_service import TTSService
from services.stt_service import STTService
from services.llm_service import LLMService
from services.emotion_service import EmotionService

# Initialize FastAPI app
app = FastAPI(title="AI Companion Orchestrator")

# Configure CORS for frontend integration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Set environment variables for services
os.environ["TTS_SERVICE_URL"] = os.environ.get("TTS_URL", "http://tts:5002")
os.environ["STT_SERVICE_URL"] = os.environ.get("STT_URL", "http://whisper:9000")

# Initialize services
llm_service = LLMService(os.environ.get("OLLAMA_URL", "http://ollama:11434"))
tts_service = TTSService()
stt_service = STTService(os.environ.get("WHISPER_API_URL", "http://whisper-stt:9000"))
emotion_service = EmotionService()

# Available modes
available_modes = ["general", "french_tutor", "coding_assistant"]
default_mode = "general"

# Initialize mode manager after LLM service
mode_manager = ModeManager(available_modes, default_mode, llm_service)

# Initialize available models list (will be populated on startup)
available_models = []
default_model = os.getenv("DEFAULT_MODEL", "llama2")

# Models will be dynamically loaded from Ollama on startup
@app.on_event("startup")
async def startup_event():
    global available_models
    try:
        # Fetch available models from Ollama
        models = await llm_service.get_available_models()
        print(f"Available models: {models}")
        if models:
            available_models = models
            print(f"Loaded {len(models)} available models from Ollama: {models}")
        else:
            # Fallback if no models could be fetched
            fallback_models = os.getenv("AVAILABLE_MODELS", "llama2,tinyllama:latest,mistral").split(",")
            available_models = fallback_models
            print(f"Using fallback model list: {fallback_models}")
    except Exception as e:
        print(f"Error loading models from Ollama: {e}")
        # Fallback to environment variable
        fallback_models = os.getenv("AVAILABLE_MODELS", "llama2,tinyllama:latest,mistral").split(",")
        available_models = fallback_models
        print(f"Using fallback model list: {fallback_models}")

# Initialize mode manager
available_modes = os.getenv("AVAILABLE_MODES", "general,french_tutor,motivator,chill_buddy").split(",")
default_mode = os.getenv("DEFAULT_COMPANION_MODE", "general")
mode_manager = ModeManager(available_modes, default_mode, llm_service)

# Data models
class TextInput(BaseModel):
    text: str
    mode: Optional[str] = None
    model: Optional[str] = None
    generate_audio: bool = True
    user_id: Optional[str] = "default_user"

class AudioInput(BaseModel):
    audio_data: bytes
    mode: Optional[str] = None
    model: Optional[str] = None
    generate_audio: bool = True
    user_id: Optional[str] = "default_user"

class CompanionResponse(BaseModel):
    text: str
    audio_url: Optional[str] = None
    emotion: str = "neutral"

class ModeInfo(BaseModel):
    name: str
    description: str
    active: bool = False
    id: str

class ModelInfo(BaseModel):
    id: str
    name: str
    active: bool = False

# Websocket clients
active_connections: Dict[str, WebSocket] = {}

@app.get("/")
async def root():
    return {"message": "AI Companion Orchestrator API"}

@app.get("/health")
def health_check():
    return {"status": "ok"}

@app.get("/audio/{filename}")
async def get_audio(filename: str):
    """Serve audio files that were generated by the TTS service."""
    try:
        file_path = os.path.join("/app", filename)
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"Audio file {filename} not found")
            
        return FileResponse(
            file_path,
            media_type="audio/wav",
            headers={"Content-Disposition": f"attachment; filename={filename}"})
    except Exception as e:
        print(f"Error serving audio file: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error serving audio: {str(e)}") from e

@app.get("/modes", response_model=List[ModeInfo])
async def get_available_modes():
    print("Available modes AMRO:", mode_manager.get_available_modes())
    return mode_manager.get_available_modes()

@app.post("/mode/{mode_name}")
async def set_active_mode(mode_name: str):
    try:
        mode_manager.set_active_mode(mode_name)
        return {"message": f"Mode set to {mode_name}"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/models", response_model=List[ModelInfo])
async def get_available_models():
    # Create model info objects
    models = []
    print(available_models)
    for model_id in available_models:
        # Format display name (capitalize and remove special chars)
        display_name = model_id.split(":")[0].replace("_", " ").title()
        if ":" in model_id:
            # Add version info if present
            version = model_id.split(":")[1]
            display_name += f" ({version})"
            
        models.append(ModelInfo(
            id=model_id,
            name=display_name,
            active=(model_id == llm_service.current_model)
        ))
    return models

@app.post("/model/{model_id}")
async def set_active_model(model_id: str):
    try:
        if model_id not in available_models:
            raise ValueError(f"Model {model_id} not available")
            
        # Set the model in the LLM service
        llm_service.current_model = model_id
        
        return {"message": f"Model set to {model_id}"}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/chat", response_model=CompanionResponse)
async def chat_endpoint(input_data: TextInput):
    try:
        # Set mode if specified
        if input_data.mode:
            try:
                mode_manager.set_active_mode(input_data.mode)
            except ValueError:
                pass  # Ignore invalid mode
        
        # Get system prompt based on active mode
        system_prompt = mode_manager.get_active_system_prompt()
        
        # Use specified model if provided, otherwise use current_model from service
        model = input_data.model if input_data.model else llm_service.current_model
        
        # Generate LLM response
        text_response = await llm_service.generate_response(
            prompt=input_data.text,
            system_prompt=system_prompt,
            model=model
        )
        
        # Convert text to audio if requested
        audio_url = None
        audio_binary = None
        if input_data.generate_audio:
            try:
                # Get binary audio data
                audio_binary = await tts_service.text_to_speech(text_response)
                
                # For API responses, we need a URL, not binary data
                # Save binary data to a temporary file and return the URL
                if audio_binary:
                    import tempfile
                    import uuid
                    import os
                    
                    # Create a unique filename
                    filename = f"response_{int(time.time())}.wav"
                    filepath = os.path.join("/app", filename)  # Save in container
                    
                    # Write binary data to file
                    with open(filepath, "wb") as f:
                        f.write(audio_binary)
                    
                    # Return URL that can be accessed from outside the container
                    audio_url = f"/audio/{filename}"
            except Exception as tts_error:
                print(f"Error generating audio: {str(tts_error)}")
                audio_url = None
        
        return CompanionResponse(
            text=text_response,
            audio_url=audio_url,
            emotion=emotion_service.detect_emotion(text_response)
        )
    except Exception as e:
        print(f"Error in chat endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing chat: {str(e)}")


@app.post("/voice", response_model=CompanionResponse)
async def voice_endpoint(audio_data: UploadFile = File(...), 
                       model: Optional[str] = Form(None),
                       mode: Optional[str] = Form(None),
                       generate_audio: bool = Form(True),
                       user_id: str = Form("default_user")):
    try:
        # Print debugging info
        print(f"Voice endpoint called with: audio_file={audio_data.filename}, model={model}, mode={mode}")
        
        # Read the uploaded file content
        file_content = await audio_data.read()
        print(f"Read {len(file_content)} bytes from uploaded audio file")
        
        # Convert speech to text using STT service
        try:
            text = await stt_service.speech_to_text(file_content)
            print(f"STT result: '{text}'")
        except Exception as stt_error:
            print(f"STT service error: {str(stt_error)}")
            import traceback
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"STT service error: {str(stt_error)}") from stt_error
        
        if not text:
            raise HTTPException(status_code=400, detail="Could not transcribe audio")
        
        # Create TextInput with the parameters from form data
        text_input = TextInput(
            text=text, 
            mode=mode, 
            model=model,
            generate_audio=generate_audio,
            user_id=user_id
        )
        
        # Use the chat endpoint to process
        try:
            return await chat_endpoint(text_input)
        except Exception as chat_error:
            print(f"Chat endpoint error: {str(chat_error)}")
            import traceback
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"Chat processing error: {str(chat_error)}") from chat_error
    except Exception as e:
        print(f"Error in voice endpoint: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error processing voice: {str(e)}") from e
    finally:
        # Make sure to close the file
        await audio_data.close()

@app.post("/text-to-speech")
async def text_to_speech_endpoint(input_data: TextInput):
    try:
        # Generate speech audio for the input text
        audio_data = await tts_service.text_to_speech(input_data.text)
        
        # If we couldn't generate audio, return an error
        if audio_data is None:
            raise HTTPException(status_code=500, detail="Failed to generate speech")
        
        # Return the binary audio data as a streaming response
        return StreamingResponse(
            io.BytesIO(audio_data),
            media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=speech.wav"}
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating speech: {str(e)}")

@app.websocket("/ws/{user_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str):
    await websocket.accept()
    active_connections[user_id] = websocket
    
    try:
        while True:
            data = await websocket.receive_text()
            payload = json.loads(data)
            
            if "type" not in payload:
                await websocket.send_json({"error": "Missing 'type' in request"})
                continue
                
            if payload["type"] == "text":
                input_data = TextInput(
                    text=payload.get("text", ""),
                    mode=payload.get("mode", None),
                    user_id=user_id
                )
                response = await chat_endpoint(input_data)
                await websocket.send_json(response.dict())
            
            elif payload["type"] == "mode":
                try:
                    mode_name = payload.get("mode", default_mode)
                    mode_manager.set_active_mode(mode_name)
                    await websocket.send_json({"message": f"Mode set to {mode_name}"})
                except ValueError as e:
                    await websocket.send_json({"error": str(e)})
            
            else:
                await websocket.send_json({"error": f"Unknown request type: {payload['type']}"})
                
    except WebSocketDisconnect:
        if user_id in active_connections:
            del active_connections[user_id]
    except Exception as e:
        if user_id in active_connections:
            del active_connections[user_id]
        print(f"Error in websocket: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
